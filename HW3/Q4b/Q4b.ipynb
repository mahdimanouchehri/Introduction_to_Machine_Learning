{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I90qRc4S1vgm"
   },
   "source": [
    "Please read the codes and instructions given to you carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8GOy2dW19v7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "from linear import *\n",
    "from module import *\n",
    "from optimizer import *\n",
    "from relu import *\n",
    "from sgd import *\n",
    "from sigmoid import *\n",
    "from softmax_crossentropy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSbUOPo11-q1"
   },
   "source": [
    "## 1. Modules\n",
    "In this problem you see some classes for typical modules used in neural networks. These modules include:\n",
    "\n",
    "1. **`ReLU`**: ReLU activation function\n",
    "\n",
    "2. **`Sigmoid`**: Sigmoid activation function\n",
    "\n",
    "3. **`SoftmaxCrossEntropy`**: A module which represents the softmax activation function followed by a cross entropy loss function.\n",
    "\n",
    "4. **`Linear`**: Fully connected layer which multiplies the input by a weight matrix and adds a bias term to it\n",
    "\n",
    "In this problem you have to implement `forward()` and `backward()` functions of the modules in the above list. Then your implemented codes will be tested here in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKHSlGoN4aZi"
   },
   "source": [
    "### 1.1. Activation Functions\n",
    "Open files `relu.py` and `sigmoid.py`. There you can see **`ReLU`** and **`Sigmoid`** classes. \n",
    "Implement `forward()` and `backward()` functions of theses classes.\n",
    "\n",
    "Here are some tips that might help you:<br/>\n",
    "**1)** `forward()` function of the layers takes an input called `x` which is a numpy 2d-array with shape `(N, D)`. Implement the forward pass corresponding to considered function. Store the results of forward pass in `out`.<br/>\n",
    "**2)** `backward()` function of the layers takes a parameter called `dout`. `dout` is the gradient of loss w.r.t. the ouput of layer in the forward pass. Provided `dout`, you have to compute the gradient w.r.t. the inputs (i.e. `x`)  to the layer. Store the results in `dx`. <br/>\n",
    "**3)** For implementing backward pass, you may need some of the variables computed during forward pass; Save these variables in the `self.cache` attribute of the layer during forward pass and use them in the backward pass.<br/>\n",
    "**4)** Test your implementation with the following cells. You should see a small value as error between your funnctions outputs and `correct output`, `correct_dx` for each activation functions. We get error of orders 1e-8 and less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rP-EErtUyC9R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu Test Cell:\n",
      "Relative error forward pass: 1.2327591637534371e-08\n",
      "Relative error backward pass: 0.0\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                             Relu Test                                   #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N = 5\n",
    "d = 6\n",
    "x = np.random.randn(N,d)\n",
    "relu = ReLU('test')\n",
    "print('Relu Test Cell:')\n",
    "output = relu.forward(x)\n",
    "correct_output = [[0.,         0. ,        1.08179168, 0.,         0.  ,       0.   ,     ],\n",
    "                  [0.9188215 , 0.  ,       0.62649346, 0. ,        0.02885512, 0.,        ],\n",
    "                  [0.58775221, 0.75231758, 0.   ,      1.05597241 ,0.74775027, 1.06467659],\n",
    "                  [1.52012959, 0.  ,       1.85998989, 0.  ,       0. ,        0.337325  ],\n",
    "                  [1.04672873 ,0.62914334, 0.36305909, 0.5557497,  0.,         0.02369477]]\n",
    "\n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dx = relu.backward(np.ones((N,d), dtype=np.float32))\n",
    "correct_dx = [[0., 0. ,1. ,0. ,0., 0.],\n",
    "              [1., 0. ,1., 0. ,1., 0.],\n",
    "              [1. ,1. ,0. ,1., 1., 1.],\n",
    "              [1. ,0. ,1., 0., 0. ,1.],\n",
    "              [1. ,1. ,1. ,1. ,0., 1.]]\n",
    "print('Relative error backward pass:', np.linalg.norm(dx - correct_dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8bxGWZGDanc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Test Cell:\n",
      "Relative error forward pass: 1.3652868830496268e-08\n",
      "Relative error backward pass: 1.515199259019241e-08\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                             sigmoid Test                                #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N=5\n",
    "d=6\n",
    "x = np.random.randn(N,d)\n",
    "sigmoid = Sigmoid('test')\n",
    "print('Sigmoid Test Cell:')    \n",
    "output = sigmoid.forward(x)\n",
    "correct_output = [[0.4770287,  0.18795539, 0.74683289, 0.44045266, 0.37962761, 0.26849495],\n",
    "                  [0.71480192, 0.24905997, 0.65169394, 0.36319727, 0.50721328, 0.44256287],\n",
    "                  [0.64284923, 0.67968348, 0.25759572, 0.74192012, 0.6786883,  0.74358323],\n",
    "                  [0.82055756, 0.18413151, 0.86529577, 0.16817555, 0.34387488, 0.58354059],\n",
    "                  [0.74014623, 0.65229519, 0.58978075, 0.63546853, 0.25189151, 0.50592342]]\n",
    "    \n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dx = sigmoid.backward(np.ones((N,d), dtype=np.float32))\n",
    "correct_dx = [[0.24947232, 0.15262816, 0.18907352, 0.24645411, 0.23551049, 0.19640541],\n",
    "              [0.20386014, 0.1870291,  0.22698895, 0.23128501, 0.24994797, 0.24670098],\n",
    "              [0.2295941,  0.21771385, 0.19124017, 0.19147466, 0.21807049, 0.19066721],\n",
    "              [0.14724285, 0.1502271,  0.116559,   0.13989254, 0.22562495, 0.24302097],\n",
    "              [0.19232979, 0.22680617, 0.24193942, 0.23164828, 0.18844218, 0.24996491]]\n",
    "    \n",
    "print('Relative error backward pass:', np.linalg.norm(dx - correct_dx))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhU008dXEGab"
   },
   "source": [
    "### 1.2. Softmax activation with Cross Entropy loss\n",
    "\n",
    "You have to implement a numerically stable version of softmax in this problem.\n",
    "\n",
    "Open file `softmax_crossentropy.py`. There you see a class for Softmax activation with Cross Entropy loss used in neural networks.\n",
    " \n",
    "\n",
    "Implement `forward()` and `backward()` function of SoftmaxCrossentropy class corresponding to forward and backward pass of softmax activation followed by cross entropy loss. Test your implementation with the following functions. The order of outputs should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnm4K_L-EbJh"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                  Softmax with Cross Entropy Test                        #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N=5\n",
    "d=6\n",
    "  \n",
    "x = np.linspace(1000, 1015, num=N*d).reshape(N,d)\n",
    "y = np.random.randint(0, d, (N, ))\n",
    "    \n",
    "softmax_ce = SoftmaxCrossentropy('test')\n",
    "print('Softmax with Cross Entropy Test Cell:')    \n",
    "loss, _ = softmax_ce.forward(x, y=y)\n",
    "dx = softmax_ce.backward()\n",
    "    \n",
    "correct_loss = 1.6883967462546619\n",
    "print('Loss relative error:', np.abs(loss - correct_loss))\n",
    "    \n",
    "correct_dx = [[ 0.00636809,  0.0106818,   0.01791759,  0.03005485,  0.05041383, -0.11543615],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385],\n",
    "              [-0.19363191,  0.0106818,   0.01791759,  0.03005485,  0.05041383,  0.08456385],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385]]\n",
    "print('Gradient relative error:', np.linalg.norm(dx - correct_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY2kXfpbMwPO"
   },
   "source": [
    "### 1.3. Linear\n",
    "Open file `linear.py`. There you see a class for linear (or fully connected) layer used in neural networks.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of a linear layer.<br/>\n",
    " \n",
    "We have implemented `__init__()` constructor for you. You can see `W`, `b`, `dW`, `db` variables in this class.\n",
    "\n",
    "In the forward pass, You have to output $y=XW+b$ in which $W$ and $b$ are parameters of the layer. In the backward pass you get $\\large{\\frac{\\partial{loss}}{\\partial{y}}}$ as `dout`; You have to compute $\\large{\\frac{\\partial{loss}}{\\partial{X}}}$, $\\large{\\frac{\\partial{loss}}{\\partial{W}}}$ and $\\large{\\frac{\\partial{loss}}{\\partial{b}}}$ and save them in `dx`, `dW`, and `db`. The output of the `backward()` function will be `dx`. \n",
    "\n",
    "Note that a linear layer with the shape of weights $(D, K)$ represents a layer in a MLP with $K$ neurons (or units). In other words, it takes a $D$-dimensional input and gives back a $K$-dimensional ouput.\n",
    "\n",
    "Test your implementation with the following functions. The order of ouputs should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0FyQoJ8NiDi"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                         Linear Test                            #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "D = 4\n",
    "K = 3\n",
    "N = 5\n",
    "x = np.random.randn(N,D)\n",
    "linear = Linear('test', D, K, l2_coef=1.)\n",
    "output = linear.forward(x)\n",
    "    \n",
    "correct_output = [[-0.51242952, -1.47921276, -2.32943713],\n",
    "                  [-1.17901283, -2.60908172,  0.54809823],\n",
    "                  [ 0.74600461, -2.24752841, -1.1013558 ],\n",
    "                  [ 0.75284837,  1.80111973, -2.27011589],\n",
    "                  [ 2.03171234, -3.05396933,  1.35213333]]\n",
    "    \n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dout = np.random.randn(N, K)\n",
    "dx = linear.backward(dout)\n",
    "    \n",
    "correct_dx = [[-0.25519113, -0.09724317,  0.280189,    0.87644613],\n",
    "              [ 1.20379991, -0.78816259, -1.27930227, -4.1952743 ],\n",
    "              [-0.77808532, -0.05005675, -3.14028536, -8.02818572],\n",
    "              [ 0.95446653, -1.90375857,  1.62080372,  3.57597736],\n",
    "              [ 2.86716776, -1.39892213,  0.31786772, -0.88234943]]\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n",
    "    \n",
    "correct_dW = [[ 3.33629487, -4.43357113, -1.89100503],\n",
    "              [ 1.31103323,  2.17687036, -2.33906146],\n",
    "              [ 1.69538051, -0.89256682, -0.86018824],\n",
    "              [-0.87944724,  7.48073741, -7.0605863 ]]\n",
    "print('Relative error dw:', np.linalg.norm(linear.dW - correct_dW))\n",
    "    \n",
    "correct_db = [-1.02223284, -3.61915576, -0.16696389]\n",
    "print('Relative error db:', np.linalg.norm(linear.db - correct_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbIbV_oku_fq"
   },
   "source": [
    "## 2. Optimization Algorithm\n",
    "\n",
    "Open file `sgd.py`.<br/> \n",
    "you have to implement `SGD` class.\n",
    "\n",
    "Test your implementation with the following cells. The order of errors should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMwdZUmqRMQO",
    "outputId": "e7454185-e7fe-4eae-d03e-310677864cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W Relative error: 1.4426196588173724e-08\n",
      "b Relative error:  5.771526066321419e-09\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                           SGD Test                             #\n",
    "###########################################################################\n",
    "N, D = 5, 4\n",
    "np.random.seed(22)\n",
    "linear = Linear('test', N, D, l2_coef=1.)\n",
    "linear.dW = np.random.randn(N, D)\n",
    "linear.db = np.random.randn(D,)\n",
    "\n",
    "sgd = SGD(1e-2)\n",
    "\n",
    "sgd.update(linear)\n",
    "\n",
    "correct_W = [[-0.10241721, -1.46964209,  1.07816109, -0.24488267],\n",
    "             [-0.48024364, -1.00250896,  0.89382974, -1.0787318 ],\n",
    "             [ 0.62884208, -0.56053815,  0.03772041, -0.22940006],\n",
    "             [ 0.58673243,  0.75482687, -1.05771443,  1.06682406],\n",
    "             [ 0.74180098,  1.07106567,  1.53120796, -1.50966439]]\n",
    "correct_b = [ 1.86566377, -1.59381353, -0.62684131,  0.33332912]\n",
    "\n",
    "print('W Relative error:', np.linalg.norm(correct_W - linear.W))\n",
    "print('b Relative error: ', np.linalg.norm(correct_b - linear.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B78R9RWgKWFD"
   },
   "source": [
    "## 3. Test Network on CIFAR10\n",
    "CIFAR10 is a dataset of 50,000 32x32x3 color training images and 10,000 test images, labeled over 10 categories (labels are 0 to 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bedhflJ8RE_n"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzulAL2CTz5Y"
   },
   "source": [
    "### 3.1. Load Data\n",
    "download the dataset using cifar10.load_data() and store the results into (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lx0mp8vOfZf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val)= cifar100.load_data()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3po6F_CYOfZf"
   },
   "source": [
    "flatten the images (reshape the images to single dimension vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOFG4QMjOfZf"
   },
   "outputs": [],
   "source": [
    "X_train = # flatten each image in X_train\n",
    "X_val = # flatten each image in X_val\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm6WcPL8OfZg"
   },
   "source": [
    "convert the labels to scalers (using `np.squeeze()` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp-q_Uw2OfZg"
   },
   "outputs": [],
   "source": [
    "y_train = # squeeze y_train\n",
    "y_val = # squeeze y_val\n",
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0oq5JkMOfZg"
   },
   "source": [
    "### 3.2. Normalization\n",
    " Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsctzHrpOfZg"
   },
   "outputs": [],
   "source": [
    "print(X_train.max(), X_train.min())\n",
    "print(X_val.max(), X_val.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeiQ7zurOfZh"
   },
   "source": [
    "rgb values range from 0 to 255. normalize the data by dividing every value by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGMC2nHbOfZh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = # devide X_train by 255\n",
    "X_val = # devide X_val by 255\n",
    "print(X_train.max(), X_train.min())\n",
    "print(X_val.max(), X_val.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJNdvzNHOfZh"
   },
   "source": [
    "### 3.3. Plot Function\n",
    "This function get a history of training (loss, acc, val_loss, val_acc) as input and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q35JDeqOfZh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    losses, accs, val_losses, val_accs = history\n",
    "    x = [i for i in range(len(losses))]\n",
    "    \n",
    "    # plot for losses\n",
    "    plt.plot(x, losses, '-g', label='train')\n",
    "    plt.plot(x, val_losses, '-r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot for accuracies\n",
    "    plt.plot(x, accs, '-g', label='train')\n",
    "    plt.plot(x, val_accs, '-r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3XG5NOyOfZi"
   },
   "source": [
    "# 2. Run the model\n",
    "\n",
    "model specification:\n",
    "\n",
    "\n",
    "1.   1 hidden layer with 100 neurons (use `Linear` module)\n",
    "2.   one of the activation functions (`ReLU` or `sigmoid`)\n",
    "3.   a 10 neurons output layer (use `Linear` module)\n",
    "4.   softmax corss entropy loss (use `SoftmaxCrossentropy` module)\n",
    "\n",
    "\n",
    "\n",
    "We train the model for 10 epochs and `batch_size = 1024`. We also plot the history of training.\n",
    "\n",
    "You can use `model.add(Module)` to add a module to the model. The modules are added in order.\n",
    "It's also recommended to read the `model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_1Mw7xYOfZj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# todo : set suitable learning rate (1, 0.1, 0.01, 0.001)\n",
    "lr = #\n",
    "model = Model(SGD(learning_rate=lr))\n",
    "#################################################################################\n",
    "# todo : add the layers to the model                                            #\n",
    "#        choose suitable l2_coef for both linear layers (1e-2, 1e-3, 1e-4, 1e-5)#\n",
    "# ###############################################################################  \n",
    "\n",
    "#################################################################################\n",
    "#                             end of code                                       #\n",
    "#################################################################################\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=10)\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Part1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
